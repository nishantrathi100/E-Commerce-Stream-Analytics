{"paragraphs":[{"text":"%md\n# Purpose of this notebook \n1. It loads log files of a day to spark. \n2. It performs some analytics on it. \n3. Stores the results in cassandra.\n4. This will help us to do analytics on day to day comparison.\n\n#### PS: The script is written in scala.\n","user":"anonymous","dateUpdated":"2017-05-08T22:31:50-0700","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{"formName":"20170501","\"Date in yyyymmdd format\"":"20170501"},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Purpose of this notebook</h1>\n<ol>\n  <li>It loads log files of a day to spark.</li>\n  <li>It performs some analytics on it.</li>\n  <li>Stores the results in cassandra.</li>\n  <li>This will help us to do analytics on day to day comparison.</li>\n</ol>\n<h4>PS: The script is written in scala.</h4>\n</div>"}]},"apps":[],"jobName":"paragraph_1493971291675_2029912719","id":"20170505-010131_1073270227","dateCreated":"2017-05-05T01:01:31-0700","dateStarted":"2017-05-08T22:31:50-0700","dateFinished":"2017-05-08T22:31:50-0700","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:2258"},{"text":"\ncase class StreamLine(userid:String,url:String,location:String,timestamp:String,datestring:String)\nval logFile = sc.textFile(z.input(\"Input click stream log file path\",\"/Users/nishantrathi/projects/BIProject/SparkAnalyticsProject/logs2.log\").toString())\nval datestring = z.input(\"Date String in yyyymmdd\",\"20170501\")\nval df= logFile.map( x => x.split(\" \") ).map( x=> StreamLine(x(0),x(1),x(2),x(3),datestring.toString())).toDF()\n\ndf.createOrReplaceTempView(\"clickstreamlogs\")\nval sqlDF1 = spark.sql(\"SELECT datestring, count(*) total_traffic FROM clickstreamlogs group by datestring\").registerTempTable(\"a\")\nval sqlDF2 = spark.sql(\"select datestring, count(distinct(userid)) as no_of_users from clickstreamlogs group by datestring\").registerTempTable(\"b\")\nval sqlDF3 = spark.sql(\"select datestring, count(distinct(userid)) no_of_order_placed_attempt from clickstreamlogs where url like '%placeOrder%' group by datestring\").registerTempTable(\"c\")\nval sqlDF4 = spark.sql(\"select datestring, count(distinct(userid)) order_success from clickstreamlogs where url like '%orderSuccess%' group by datestring\").registerTempTable(\"d\")\nval sqlDF5 = spark.sql(\"select datestring, count(distinct(userid)) order_failure from clickstreamlogs where url like '%orderFailure%' group by datestring\").registerTempTable(\"e\")\n\nval sqlDF11 = spark.sql(\"select a.datestring as datestring, total_traffic, no_of_users from a INNER JOIN b ON a.datestring = b.datestring\").registerTempTable(\"ab\")\nval sqlDF12 = spark.sql(\"select c.datestring as datestring, total_traffic, no_of_users, no_of_order_placed_attempt from ab INNER JOIN c ON ab.datestring = c.datestring \").registerTempTable(\"abc\")\nval sqlDF13 = spark.sql(\"select d.datestring as datestring,total_traffic, no_of_users, no_of_order_placed_attempt, order_success from abc INNER JOIN d ON abc.datestring = d.datestring \").registerTempTable(\"abcd\")\nval sqlDF14 = spark.sql(\"select e.datestring as datestring, e.datestring as date,total_traffic, no_of_users, no_of_order_placed_attempt, order_success, order_failure from abcd INNER JOIN e ON abcd.datestring = e.datestring \")\nsqlDF14.show()\nsqlDF14.write.csv(\"/Users/nishantrathi/projects/BIProject/SparkAnalyticsProject/op_to_cassandra\")\n// INNER JOIN c INNER JOIN d INNER JOIN e where  and b.datestring = c.datestring and d.datestring = e.datestring\"\n//sqlDF14.show()\n\n//val sqlDF6 = spark.sql(\"select datestring, location, count(*) from clickstreamlogs group by datestring,location\")\n//sqlDF6.map(attributes => attributes(1) , attributes(2)).show()\n//sqlDF6.show()\n\n//sqlDF1.write.csv(\"/Users/nishantrathi/projects/BIProject/SparkAnalyticsProject/op_to_cassandra\")\n\n\n//val rows: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = sqlDF1.rdd\n//val sqlDF2 = spark.sql(\"SELECT datestring, count(distinct(userid)) as no_of_users from clickstreamlogs group by datestring\")\n//val sqlDF3 = spark.sql(\"select datestring, avg(duration) from (select datestring,userid, (max(timestamp) - min(timestamp)) duration  from clickstreamlogs group by datestring, userid) group by datestring\")\n//rows.collect()\n\n\n","user":"anonymous","dateUpdated":"2017-05-08T23:45:16-0700","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":true},"editorMode":"ace/mode/scala","editorHide":true,"tableHide":false},"settings":{"params":{"Date String in yyyymmdd":"20170510","Input click stream log file path":"/Users/nishantrathi/projects/eclipseworkspace1/ClickStreamSimulator/logs_1494308533069.log"},"forms":{"Input click stream log file path":{"name":"Input click stream log file path","displayName":"Input click stream log file path","type":"input","defaultValue":"/Users/nishantrathi/projects/BIProject/SparkAnalyticsProject/logs2.log","hidden":false,"$$hashKey":"object:2544"},"Date String in yyyymmdd":{"name":"Date String in yyyymmdd","displayName":"Date String in yyyymmdd","type":"input","defaultValue":"20170501","hidden":false,"$$hashKey":"object:2543"}}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\ndefined class StreamLine\n\nlogFile: org.apache.spark.rdd.RDD[String] = /Users/nishantrathi/projects/eclipseworkspace1/ClickStreamSimulator/logs_1494308533069.log MapPartitionsRDD[5780] at textFile at <console>:29\n\ndatestring: Object = 20170510\n\ndf: org.apache.spark.sql.DataFrame = [userid: string, url: string ... 3 more fields]\n\nwarning: there was one deprecation warning; re-run with -deprecation for details\n\nsqlDF1: Unit = ()\n\nwarning: there was one deprecation warning; re-run with -deprecation for details\n\nsqlDF2: Unit = ()\n\nwarning: there was one deprecation warning; re-run with -deprecation for details\n\nsqlDF3: Unit = ()\n\nwarning: there was one deprecation warning; re-run with -deprecation for details\n\nsqlDF4: Unit = ()\n\nwarning: there was one deprecation warning; re-run with -deprecation for details\n\nsqlDF5: Unit = ()\n\nwarning: there was one deprecation warning; re-run with -deprecation for details\n\nsqlDF11: Unit = ()\n\nwarning: there was one deprecation warning; re-run with -deprecation for details\n\nsqlDF12: Unit = ()\n\nwarning: there was one deprecation warning; re-run with -deprecation for details\n\nsqlDF13: Unit = ()\n\nsqlDF14: org.apache.spark.sql.DataFrame = [datestring: string, date: string ... 5 more fields]\n+----------+--------+-------------+-----------+--------------------------+-------------+-------------+\n|datestring|    date|total_traffic|no_of_users|no_of_order_placed_attempt|order_success|order_failure|\n+----------+--------+-------------+-----------+--------------------------+-------------+-------------+\n|  20170510|20170510|        55479|       3954|                      1637|         1334|          303|\n+----------+--------+-------------+-----------+--------------------------+-------------+-------------+\n\n"}]},"apps":[],"jobName":"paragraph_1493971382346_848103660","id":"20170505-010302_1008212329","dateCreated":"2017-05-05T01:03:02-0700","dateStarted":"2017-05-08T23:45:16-0700","dateFinished":"2017-05-08T23:47:27-0700","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2259"},{"text":"%sh\ncqlsh -e \"copy ecommerceanalytics.dailysummary1 (datestring, date, total_traffic, no_of_users,no_of_order_placed_attempt, order_success, order_failure ) from '/Users/nishantrathi/projects/BIProject/SparkAnalyticsProject/op_to_cassandra/*.csv' ;\"\nrm -rf /Users/nishantrathi/projects/BIProject/SparkAnalyticsProject/op_to_cassandra","user":"anonymous","dateUpdated":"2017-05-08T23:47:34-0700","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"sh","editOnDblClick":false},"editorMode":"ace/mode/sh","editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Using 3 child processes\n\nStarting copy of ecommerceanalytics.dailysummary1 with columns [datestring, date, total_traffic, no_of_users, no_of_order_placed_attempt, order_success, order_failure].\nProcessed: 1 rows; Rate:       3 rows/s; Avg. rate:       3 rows/s\r\rProcessed: 1 rows; Rate:       2 rows/s; Avg. rate:       2 rows/s\r\r\n1 rows imported from 200 files in 0.425 seconds (0 skipped).\n"}]},"apps":[],"jobName":"paragraph_1493976312635_2043550188","id":"20170505-022512_1553439261","dateCreated":"2017-05-05T02:25:12-0700","dateStarted":"2017-05-08T23:47:34-0700","dateFinished":"2017-05-08T23:47:35-0700","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2260"},{"text":"val sqlDF6 = spark.sql(\"select concat (datestring,'=', location) dsloc,  datestring, location, count(*) traffic from clickstreamlogs group by datestring,location\").registerTempTable(\"f\")\nval sqlDF7 = spark.sql(\"select concat (datestring,'=', location) dsloc,  datestring, location, count(distinct(userid)) no_of_users from clickstreamlogs group by datestring,location \").registerTempTable(\"g\")\nval sqlDF8 = spark.sql(\"select concat (datestring,'=', location) dsloc,  datestring, location, count(*) order_failure from clickstreamlogs where url like '%orderFailure%' group by datestring,location \").registerTempTable(\"h\")\nval sqlDF9 = spark.sql(\"select concat (datestring,'=', location) dsloc,  datestring, location, count(*) order_success from clickstreamlogs where url like '%orderSuccess%' group by datestring,location \").registerTempTable(\"i\")\nval sqlDF61 = spark.sql(\"select f.dsloc dsloc, f.datestring datestring, f.location location, traffic, no_of_users from f INNER JOIN g on f.dsloc = g.dsloc\").registerTempTable(\"fg\")\nval sqlDF62 = spark.sql(\"select h.dsloc dsloc, h.datestring datestring, h.location location, traffic, no_of_users, order_failure from fg INNER JOIN h on fg.dsloc = h.dsloc\").registerTempTable(\"fgh\")\nval sqlDF63 = spark.sql(\"select i.dsloc dsloc, i.datestring datestring, i.location location, traffic, no_of_users, order_failure, order_success from fgh INNER JOIN i on fgh.dsloc = i.dsloc\")\nsqlDF63.show()\nsqlDF63.write.csv(\"/Users/nishantrathi/projects/BIProject/SparkAnalyticsProject/op_to_cassandra\")\n","user":"anonymous","dateUpdated":"2017-05-08T23:47:39-0700","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala","editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nwarning: there was one deprecation warning; re-run with -deprecation for details\n\nsqlDF6: Unit = ()\n\nwarning: there was one deprecation warning; re-run with -deprecation for details\n\nsqlDF7: Unit = ()\n\nwarning: there was one deprecation warning; re-run with -deprecation for details\n\nsqlDF8: Unit = ()\n\nwarning: there was one deprecation warning; re-run with -deprecation for details\n\nsqlDF9: Unit = ()\n\nwarning: there was one deprecation warning; re-run with -deprecation for details\n\nsqlDF61: Unit = ()\n\nwarning: there was one deprecation warning; re-run with -deprecation for details\n\nsqlDF62: Unit = ()\n\nsqlDF63: org.apache.spark.sql.DataFrame = [dsloc: string, datestring: string ... 5 more fields]\n+--------------------+----------+-------------+-------+-----------+-------------+-------------+\n|               dsloc|datestring|     location|traffic|no_of_users|order_failure|order_success|\n+--------------------+----------+-------------+-------+-----------+-------------+-------------+\n|    20170510=HOUSTON|  20170510|      HOUSTON|   3747|        258|           15|           84|\n|    20170510=DETROIT|  20170510|      DETROIT|   3659|        267|           19|           85|\n|  20170510=LAS_VEGAS|  20170510|    LAS_VEGAS|   3163|        232|           10|           83|\n|    20170510=SEATTLE|  20170510|      SEATTLE|   3362|        235|           22|           80|\n|   20170510=NEW_YORK|  20170510|     NEW_YORK|   3561|        244|           20|           90|\n|     20170510=DALLAS|  20170510|       DALLAS|   3527|        243|           11|           90|\n|20170510=JACKSONV...|  20170510| JACKSONVILLE|   3460|        259|           19|           92|\n|    20170510=CHICAGO|  20170510|      CHICAGO|   3537|        249|           25|           85|\n|20170510=SAN_ANTONIO|  20170510|  SAN_ANTONIO|   3209|        227|           23|           66|\n|  20170510=SAN_DIEGO|  20170510|    SAN_DIEGO|   3539|        246|           18|           72|\n|20170510=SAN_FRAN...|  20170510|SAN_FRANCISCO|   3241|        229|           25|           88|\n|20170510=PHILADEL...|  20170510| PHILADELPHIA|   3220|        224|           16|           85|\n|    20170510=PHOENIX|  20170510|      PHOENIX|   3294|        243|           23|           73|\n|     20170510=BOSTON|  20170510|       BOSTON|   3801|        261|           18|           88|\n|20170510=LOS_ANGELES|  20170510|  LOS_ANGELES|   3817|        292|           19|           98|\n|   20170510=SAN_JOSE|  20170510|     SAN_JOSE|   3342|        245|           20|           75|\n+--------------------+----------+-------------+-------+-----------+-------------+-------------+\n\n"}]},"apps":[],"jobName":"paragraph_1493978405054_-454188013","id":"20170505-030005_570306141","dateCreated":"2017-05-05T03:00:05-0700","dateStarted":"2017-05-08T23:47:39-0700","dateFinished":"2017-05-08T23:49:26-0700","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2261"},{"text":"%sh\ncqlsh -e \"copy ecommerceanalytics.dailysummary2 (dsloc, datestring, location, traffic, no_of_users, order_failure, order_success) from '/Users/nishantrathi/projects/BIProject/SparkAnalyticsProject/op_to_cassandra/*.csv' ;\"\nrm -rf /Users/nishantrathi/projects/BIProject/SparkAnalyticsProject/op_to_cassandra","user":"anonymous","dateUpdated":"2017-05-08T23:49:39-0700","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"sh","editOnDblClick":false},"editorMode":"ace/mode/sh"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Using 3 child processes\n\nStarting copy of ecommerceanalytics.dailysummary2 with columns [dsloc, datestring, location, traffic, no_of_users, order_failure, order_success].\nProcessed: 16 rows; Rate:      51 rows/s; Avg. rate:      51 rows/s\r\rProcessed: 16 rows; Rate:      25 rows/s; Avg. rate:      38 rows/s\r\r\n16 rows imported from 200 files in 0.419 seconds (0 skipped).\n"}]},"apps":[],"jobName":"paragraph_1494022548055_2105194643","id":"20170505-151548_962725150","dateCreated":"2017-05-05T15:15:48-0700","dateStarted":"2017-05-08T23:49:39-0700","dateFinished":"2017-05-08T23:49:40-0700","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2262"},{"text":"\n\nval sqlDF21 = spark.sql(\"select concat (datestring,'=', location, '=', url) dslocurl,  datestring, location, url, count(*)  traffic from clickstreamlogs where url like '%item%ITEM%' group by datestring,location,url\")\nsqlDF21.write.csv(\"/Users/nishantrathi/projects/BIProject/SparkAnalyticsProject/op_to_cassandra\")\nsqlDF21.show()","user":"anonymous","dateUpdated":"2017-05-08T23:49:44-0700","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nsqlDF21: org.apache.spark.sql.DataFrame = [dslocurl: string, datestring: string ... 3 more fields]\n+--------------------+----------+-------------+------------+-------+\n|            dslocurl|datestring|     location|         url|traffic|\n+--------------------+----------+-------------+------------+-------+\n|20170510=PHOENIX=...|  20170510|      PHOENIX|\\item\\ITEM13|     82|\n|20170510=SAN_ANTO...|  20170510|  SAN_ANTONIO| \\item\\ITEM2|     72|\n|20170510=CHICAGO=...|  20170510|      CHICAGO| \\item\\ITEM5|     97|\n|20170510=DETROIT=...|  20170510|      DETROIT| \\item\\ITEM3|    103|\n|20170510=SAN_FRAN...|  20170510|SAN_FRANCISCO|\\item\\ITEM13|     84|\n|20170510=SEATTLE=...|  20170510|      SEATTLE|\\item\\ITEM23|     88|\n|20170510=CHICAGO=...|  20170510|      CHICAGO|\\item\\ITEM20|     88|\n|20170510=PHOENIX=...|  20170510|      PHOENIX|\\item\\ITEM17|     79|\n|20170510=BOSTON=\\...|  20170510|       BOSTON| \\item\\ITEM1|     97|\n|20170510=SEATTLE=...|  20170510|      SEATTLE|\\item\\ITEM21|     78|\n|20170510=DALLAS=\\...|  20170510|       DALLAS|\\item\\ITEM23|     94|\n|20170510=SAN_JOSE...|  20170510|     SAN_JOSE|\\item\\ITEM12|     86|\n|20170510=LOS_ANGE...|  20170510|  LOS_ANGELES|\\item\\ITEM12|     97|\n|20170510=HOUSTON=...|  20170510|      HOUSTON|\\item\\ITEM18|     87|\n|20170510=DALLAS=\\...|  20170510|       DALLAS|\\item\\ITEM18|     75|\n|20170510=JACKSONV...|  20170510| JACKSONVILLE| \\item\\ITEM2|    105|\n|20170510=HOUSTON=...|  20170510|      HOUSTON|\\item\\ITEM16|     95|\n|20170510=LOS_ANGE...|  20170510|  LOS_ANGELES| \\item\\ITEM8|    106|\n|20170510=LAS_VEGA...|  20170510|    LAS_VEGAS|\\item\\ITEM21|     76|\n|20170510=NEW_YORK...|  20170510|     NEW_YORK| \\item\\ITEM6|     98|\n+--------------------+----------+-------------+------------+-------+\nonly showing top 20 rows\n\n"}]},"apps":[],"jobName":"paragraph_1494026066890_-1783299979","id":"20170505-161426_1977460438","dateCreated":"2017-05-05T16:14:26-0700","dateStarted":"2017-05-08T23:49:44-0700","dateFinished":"2017-05-08T23:50:00-0700","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2263"},{"text":"%sh\ncqlsh -e \"copy ecommerceanalytics.dailysummary3 (dslocurl, datestring, location, url,traffic) from '/Users/nishantrathi/projects/BIProject/SparkAnalyticsProject/op_to_cassandra/*.csv' ;\"\nrm -rf /Users/nishantrathi/projects/BIProject/SparkAnalyticsProject/op_to_cassandra","user":"anonymous","dateUpdated":"2017-05-08T23:50:36-0700","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"sh","editOnDblClick":false},"editorMode":"ace/mode/sh"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Using 3 child processes\n\nStarting copy of ecommerceanalytics.dailysummary3 with columns [dslocurl, datestring, location, url, traffic].\nProcessed: 400 rows; Rate:    1181 rows/s; Avg. rate:    1180 rows/s\r\rProcessed: 400 rows; Rate:     590 rows/s; Avg. rate:     899 rows/s\r\r\n400 rows imported from 200 files in 0.445 seconds (0 skipped).\n"}]},"apps":[],"jobName":"paragraph_1494028715699_-1075625508","id":"20170505-165835_1154836156","dateCreated":"2017-05-05T16:58:35-0700","dateStarted":"2017-05-08T23:50:36-0700","dateFinished":"2017-05-08T23:50:37-0700","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2264"},{"title":"Some dummy data insertion into cassandra","text":"%cassandra\n\n\ninsert into dailysummary1 (datestring, date, no_of_order_placed_attempt, no_of_users, order_failure, order_success, total_traffic) values ('20170501', '20170501', 9239, 25896, 197, 9042, 95577 );\ninsert into dailysummary1 (datestring, date, no_of_order_placed_attempt, no_of_users, order_failure, order_success, total_traffic) values ('20170502', '20170502', 3267, 14054, 67, 3200, 43656 );\ninsert into dailysummary1 (datestring, date,  no_of_order_placed_attempt, no_of_users, order_failure, order_success, total_traffic) values ('20170503', '20170503',6232, 21248, 131, 6101, 65537 );\ninsert into dailysummary1 (datestring, date,  no_of_order_placed_attempt, no_of_users, order_failure, order_success, total_traffic) values ('20170504', '20170504', 9942, 29898, 297, 9645, 93537 );\ninsert into dailysummary1 (datestring, date,  no_of_order_placed_attempt, no_of_users, order_failure, order_success, total_traffic) values ('20170505', '20170505', 563, 1043, 42, 521, 9012 );\ninsert into dailysummary1 (datestring, date,  no_of_order_placed_attempt, no_of_users, order_failure, order_success, total_traffic) values ('20170506', '20170506', 6992, 19343, 191, 6801, 72312 );\ninsert into dailysummary1 (datestring, date,  no_of_order_placed_attempt, no_of_users, order_failure, order_success, total_traffic) values ('20170507', '20170507', 19239, 41248, 1197, 18042, 125545 );\ninsert into dailysummary1 (datestring, date,  no_of_order_placed_attempt, no_of_users, order_failure, order_success, total_traffic) values ('20170508', '20170508', 3676, 17248, 77, 3599, 67321 );\n\n","user":"anonymous","dateUpdated":"2017-05-08T22:32:13-0700","config":{"colWidth":12,"enabled":false,"results":{},"editorSetting":{"language":"text","editOnDblClick":false},"editorMode":"ace/mode/undefined","editorHide":false,"title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"\n\n<div class=\"container\">\n<div class=\"row text-center\">\n<h4>No Result</h4>\n</div>\n<br/>\n    <div class=\"row\">\n    <div class=\"col-md-3\"></div>\n    <div class=\"col-md-6 col-offset-md-3 table-responsive table-bordered\">\n        <table class=\"table\">\n            <caption><h5>Last query execution info</h5></caption>\n            <thead>\n                <tr>\n                    <th>Info</th>\n                    <th>Value</th>\n                </tr>\n            </thead>\n            <tbody>\n                <tr>\n                    <td>Statement</td>\n                    <td>insert into dailysummary1 (datestring, date,  no_of_order_placed_attempt, no_of_users, order_failure, order_success, total_traffic) values ('20170508', '20170508', 3676, 17248, 77, 3599, 67321 );</td>\n                </tr>\n                <tr>\n                    <td>Achieved Consistency</td>\n                    <td>N/A</td>\n                </tr>\n                <tr>\n                    <td>Tried Hosts</td>\n                    <td>localhost127.0.0.1:9042</td>\n                </tr>\n                <tr>\n                    <td>Queried Hosts</td>\n                    <td>localhost127.0.0.1:9042</td>\n                </tr>\n                <tr>\n                    <td>Schema In Agreement</td>\n                    <td>true</td>\n                </tr>\n            </tbody>\n        </table>\n    </div>\n    </div>\n</div>"}]},"apps":[],"jobName":"paragraph_1494029505565_-1853087544","id":"20170505-171145_1375505048","dateCreated":"2017-05-05T17:11:45-0700","dateStarted":"2017-05-08T22:31:50-0700","dateFinished":"2017-05-08T22:31:50-0700","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2265"},{"text":"%cassandra\n","user":"anonymous","dateUpdated":"2017-05-08T22:32:06-0700","config":{"colWidth":12,"enabled":false,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/undefined"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1494124094836_344090396","id":"20170506-192814_1342699204","dateCreated":"2017-05-06T19:28:14-0700","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2266"}],"name":"Daily Log Analyzer & Uploader","id":"2CEGCT4WC","angularObjects":{"2CFZ99VB7:shared_process":[],"2CFQDA7N6:shared_process":[],"2CFEGM817:shared_process":[],"2CH38FHDK:shared_process":[],"2CH234Q92:shared_process":[],"2CFDTSWJ5:shared_process":[],"2CHFK8GB7:shared_process":[],"2CH1CMA3H:shared_process":[],"2CH31N3YB:shared_process":[],"2CFP5E9CW:shared_process":[],"2CGJ8UE9W:shared_process":[],"2CHYR7DEM:shared_process":[],"2CEE3CDBD:shared_process":[],"2CH414GMZ:shared_process":[],"2CJABYPDQ:shared_process":[],"2CGCWRTT7:shared_process":[],"2CEZT654A:shared_process":[],"2CH4S33P7:shared_process":[],"2CFSZVEV9:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}